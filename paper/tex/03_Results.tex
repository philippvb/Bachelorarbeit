\chapter{Results}
Even with such a simple formulation, there are many different variations that
can be tested. First is the influence of the strength and width parameter of the
distance function, which will be tested in section \ref{res:Hyperparameters}. We
will also briefly test different kernels besides the RBF Kernel from equation
\ref{eq:RBF} in section \ref{res:Kernel}. In section \ref{res:Multiple}, we will
use more than one checkpoint and will also try to combine these to a global one.
The interaction of training hyperparameters like learning rate and epochs will
be discussed in section \ref{res:Training}. Finally, the usage for ensemble
methods will be tested.

\section{Baseline}
For the baseline, we use the hyperparameters as defined in section
\ref{sub:Hyperparameters}. Figure \ref{fig:Results_baseline} shows the result,
if we add a checkpoint after 100 epochs of training. For the validation accuray,
we can see that it stays similar to the accuracy without the distance function.
If we plot the distance the network gets to the checkpoint however, we can see
that the network trained with distance function clearly increases its distance
more than the one trained without. Furthermore, the red plot follows the shape
we would expect from the distance function. At the beginning, the distance
increases relatively fast, as can be expected from the high gradient of the RBF
Kernel. But as the distance increases, its gradient becomes smaller, similar to
the gradient of the Kernel. The blue plot in contrast follows an almost linear
curve, whith a much smaller gradient. Therefore, the additional term succeeds in
the initial goal, which was to distance from the checkpoint. Note that despite
the validation accuracy beeing in an area of convergence, SGD even without
distance function keeps on walking and never truly stops at a point.

\begin{figure}[h]\label{fig:Results_baseline}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm,
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.8,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_distance0.csv};
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.8,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/ResNet32_baseline_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/ResNet32_baseline_distance0.csv};
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained without distance function (red) and with distance function (blue).}
    \end{center}
\end{figure}


[TODO: add gradient size to show that circular path]




\section{Distance function Hyperparameters}\label{res:Hyperparameters}
\subsection{strength}
Recall section \ref{eq:Loss_strength}, where we added strength as a
hyperparameter, controlling the influence of the distance function. As the RBF
Kernel is bound between 0 and 1, the strength hyperparameter will increase that
bound between 0 and the strength value.

For higher strength values, the validation accuracy receives an initial drop,
which is larger the higher the strength parameter value. This may be due to the
increased influence of the strength function. Each distance between the
parameters and the checkpoint starts at 0, therefore the values of the
exponential function starts at $strength \cdot 1$. A higher strength value will
lead to a higher influence on the loss function, and consequently the gradient.
The gradient of the cross-entropy loss will be insignificant, and the optimizer
will update the weights without regards to the validation accuracy, hence the
drop at the beginning. The distance plot reflects this behaviour, as we can see
an larger increase in distance for larger strength values. [add from wedge paper]

After the initial step however, the distance quickly converges. This is due to
the fact that after the initial increase, the value of the Kernel quickly
reaches 0. [TODO: Sow plot of strength]. Therefore, the distance Kernel becomes
insignificant again, and the Cross-Entropy loss is followed. The validation
accuracy reflects this behaviour, as after the initial drop the accuracy
recovers to the level of before. This provides additional insight in the loss
landscape. As the network distances from it`s current position on the loss
landscpae, but is still able to reach high accuracy, there have to be areas of
high accuracy everywhere on the landscape. This is in consonance to other
research in this field, as discussed in section \ref{loss_landscape}.

As a result, the value of the strength doesn't matter in long term at least. It
merely increases the distance to the checkpoint by defining how long the
distance term is important to the loss function. But after that, the normal loss
is followed again. As the last section and literature suggests, area of low loss
and high accuray can be found everywhere. Therefore, the same accuracy as before
can be reached, no matter the value of the strength. On the other hand, it is
also unlikely that it will outperform the last best value, as new areas are not
generally better than other.

basic results:
- if higher strength factor, distance rises more quickly
- then stays in this area of distance
- validation accuracy drops because first influenence of distance




\begin{figure}[h]\label{fig:Results_strength}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            y label style={at={(axis description cs:0.1,0)},anchor=south},
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.6,
            legend pos= south east,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e1_validation_acuracy.csv};
            \addlegendentry{$s=0.1$}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            \addlegendentry{$s=1$}
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e2_validation_acuracy.csv};
            \addlegendentry{$s=10$}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e3_validation_acuracy.csv};
            \addlegendentry{$s=100$}

            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                % y label style={at={(axis description cs:0.,0)},anchor=south},
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e1_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e2_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e3_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % x label style={at={(axis description cs:1.5,0)},anchor=west},
            xlabel=Epoch, % Set the labels
            % ylabel=Validation Accuracy,
            ymin=0.6,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e1_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e2_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e3_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                %xlabel=Epoch, % Set the labels
                %ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e1_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e2_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e3_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained without distance function (red) and with distance function (blue).}
    \end{center}
\end{figure}

\subsection{width}
We have seen in chapter \ref{distance_function}, how the width $\sigma$
influences the distance function: The larger $\sigma$ gets, the wider it
becomes. Therefore, a larger $\sigma$ should result in the network distancing
further from it's checkpoint than for smaller values. That's exactly what can be
seen in figure \ref{}: For an $\sigma$ of 0.1, the distance follows the one of
network trained without distance function, due to the RBF Kernel quickly
becoming 0. The larger the width, the slower the values of the RBF Kernel
decrease for further distance. Therefore, the gradient of the distance function
will stay quite large, pushing the network further away. This can be seen for
higher $\sigma$ values, where the distance increases further.


Initially however, the distance plot doesn't reflect this behaviour. The curve
for $\sigma = 0.01$ has a higher distance value than for $\sigma = 0.001$.
That's because the gradient of the RBF Kernel is higher for small distances, the
smaller $\sigma$. But for larger distances, this effect flips with the gradient
of the larger width staying higher for more epochs. Therefore, the curves
intersect after additional epochs. The smaller width converges, while the larger
stays constant until eventually reaching it`s area of convergence. The
intersection happens later the larger the width, for $\sigma = 0.0001$, only
after 207 epochs.

This behaviour however has no influence on the validation accuracy, which stays
the same for every width, in contrast to the strength. This may be due to the
fact, that a larger width does in fact decreases the size of the gradient. A
smaller gradient over more epochs will be added to the gradient of the
Cross-Entropy Loss. The optimizer therefore nearly keeps on following the normal
gradient and descending the valley of low loss, with a small but steady push to
increase the distance to the checkpoint. Therefore, even if the network is
allowed to move quite freely on the loss surface, it finds areas of low loss and
high accuracy in distant areas of the checkpiont, meaning there has to be good
local minima everywhere on the landscape.

[mention witdh e-3 good behaviour in terms of tradeoff]

\begin{figure}[h]\label{fig:Results_strength}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.7,
            legend pos = south east,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e1_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=1$}
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e2_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=10$}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=100$}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e4_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=1000$}

            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e1_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e2_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e4_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.7,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_validation_acuracy.csv};

            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained with different widths of the distance function.}
    \end{center}
\end{figure}








results: - wider kernel leads to higher value of
distance - maybe slower due to smaller gradient - no difference in validation
accuracy - if so, should prove that in every area exists low loss regions -
accuracy doesn't decrease -> there are valleys leading away from current point
with high accuracy - 







\section{Multiple Checkpoint}\label{res:Multiple}
idea: distance from multiple rather than 1 place
\subsection{multiple}
For multiple checkpoints, we can see the same pattern as in section
\ref{res:Hyperparameters}. For low strength, we experience no effect at all. If
we increase the strength there is an initial drop in the accuracy, but with
further epochs, this recovers to the baseline accuracy. Furthermore, it even
tops the baseline, with a small but increasing margin for every restart that is
performed. The pattern is quite similar to warm restarts, where for every
restart the reached accuracy increases. For a in depth comparison, see section
\ref{res:Learning rate}.

\begin{figure}[h]\label{fig:Results_multiple}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 1,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch,
            ylabel=Validation Accuracy,
            ymin=0.6,
            ymax=1,
            legend pos = south east,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_validation_acuracy.csv};
                \addlegendentry{$s=1$}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f10_validation_acuracy.csv};
                \addlegendentry{$s=10$}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f100_validation_acuracy.csv};
                \addlegendentry{$s=100$}
            
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            yticklabels={,,},
            ymin=0.6,
            ymax=1,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_validation_acuracy.csv};
            % [TODO: add ResNet Multiple]

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained with multiple checkpoints. [TODO: add for ResNet]}
    \end{center}
\end{figure}

The influence on the distance to the checkpoints is more interesting however. In
general, an additional checkpoint seems to decrease the distance to the others.
This is quite counterintuitive at first glance. Suppose for one parameter, we
have checkpoint value $a$ and create a new one at our current value $b<a$. To
increase the distance to both, we would just have to walk in direction $c<b$. As
we have seen in previous section, this would be sufficient as areas of low error
exist everywhere. But why is the distance then decreased for our first
checkpoint? The reason might be the $L_2$ regularization, as it restricts the
weights to small sizes. At some point, it might be less costly to decrease the
weights again for a smaller $L_2$, traded off against a bit smaller distance to
other checkpoints. This restriction in weight space is probably why new terms
lead to a decrease of the distance to existing checkpoints.
[maybe long term convergence to each other]

\begin{figure}[h]\label{fig:Results_multiple_distance}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=3 by 1,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=5cm
            ]

            \nextgroupplot[
            title=checkpoint 1,
            grid=major, 
            grid style={dashed,gray!30},
            %x label style={at={(axis description cs:1.5,0)},anchor=north},
            % y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},
            %xlabel=Epoch,
            ylabel=distance,
            ylabel near ticks,
            ymax=8000,
            xmin=140]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f10_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f100_distance0.csv};

            \nextgroupplot[
            title=checkpoint 2,
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch,
            yticklabels={,,}
            ymax=8000,
            xmin=290]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_distance1.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f10_distance1.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f100_distance1.csv};

            \nextgroupplot[
            title=checkpoint 3,
            grid=major, 
            grid style={dashed,gray!30},
            %xlabel=Epoch,
            yticklabels={,,},
            ymax=8000,
            legend pos = outer north east,
            xmin=440]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_distance2.csv};
                \addlegendentry{$s=1$}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f10_distance2.csv};
                \addlegendentry{$s=10$}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f100_distance2.csv};
                \addlegendentry{$s=100$}

            \end{groupplot}
        \end{tikzpicture}
        \caption{Distance plots for MobileNetV2 (upper) and ResNet32 (lower) trained with multiple checkpoints and different strength values. [TODO: add for ResNet]}
    \end{center}
\end{figure}

In short term however, new checkpoint produce quite different influence on the
distance to existing ones. For a strength of $s=1$ we can see a slight decrease
which turns into a longer increase until the next checkpoint. For $s=10$, we an
aprupt decrease with an small peak after. For $s=100$, there only is a peak at
the beginning which quickly diminishes. Noticeable, this pattern repeats for the
other checkpoints and therefore seems unlikely to be a coincidence. The size of
these hills can be tracked to the size of the strength. Larger strength
introduces a larger gradient and therefore step size at the beginning, making
the hills larger. This doesn't explain the different orientation of these hills.
For the case of $s=100$, the initial increase of distance seems reasonable. As
the value of the distance function is not 0, the gradient should face in
direction away from the last checkpoint. A new checkpoint introduces a gradient
boost. As momentum preserves the old gradient, it will acclerate in direction
away from the first and the second checkpoint, therefore leading to a hill.

By this explanation, the pattern should repeat for the other cases however. It
may be argued, that this pattern would still exist for other values of s.
However, it is only recongnizable in the case of large s.




- only effect for larger strength
- initial decrease, similar to starts
- reaches slightly lower train loss, but not better validation accuracy
- have to take longer baseline run to see if it comes over baseline accuracy

- for new checkpoints effect on distance of current, at the moment not consistent -> has to check for average
- in general, seems like a slight decrease but same pattern again for single checkpoint, but others show same general pattern
- explanation idea: new kernel pushes gradient size -> increase or decrease (should be an icrease because momentum???)

- gradient size becomes smaller for distance term -> maybe hint that SGD wants
to stay in that area and distance term tries to push out -> gradient direction
would be interesting (but too far for this work)



- show that multiple minima all increase distance
- show how it can be merged to a large one

- increase factor to show differences, also Ã¼berleitung to warm restarts
\subsection{merge}
A slightly different approach is to merge the checkpoints, described in section
\ref{sub:Multiple_checkpoints}.
[TODO: add more networks for merge]









\section{Training Hyperparameters}\label{res:Training}
\subsection{lr}\label{res:Learning rate}
\subsubsection{Scheduler}
Learning rate Scheduler were introduced in section \ref{sub:Learing_rate_decay},
where the learning rate was not held constant, but decayed over time. One futher
addition were warm restarts, where after the decay we set the learning rate back
up to the inital and repeat this cycle several times.

This procedure results in an increase of validation accuracy. We use a step
decay scheduler with $\gamma = 0.1$ for every 50 epochs, and a warm restart
after 150 epochs. Additionally, a cosine decay is used as in
\cite{loshchilov2016sgdr}. Figure \ref{fig:Results_scheduler} shows the
validation accuracy. We can see, that schedulers outperform a fixed learning
rate. Furthermore, the maximum accuracy of every cycle slightly increases, until
eventually coming to convergence. We can clearly see the warm restarts, as the
accuracy drops due to the high learning rate.

In section \ref{sub:cosine_decay}, the underlying idea leading to this
improvement was discussed: High learning rates should lead to exploration and
convergence to wider and more stable convergence.


\begin{figure}[h]\label{fig:Results_scheduler}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.75,
            legend pos = south east,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_validation_acuracy.csv};
                \addlegendentry{fixed lr}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/MobileNetV2_scheduler_step_validation_acuracy.csv};
                \addlegendentry{step decay}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/MobileNetV2_scheduler_cosine_validation_acuracy.csv};
                \addlegendentry{cosine decay}
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/MobileNetV2_scheduler_step_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/MobileNetV2_scheduler_cosine_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.7,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_validation_acuracy.csv};

            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained with learning rate schedulers. [TODO: add for ResNet]}
    \end{center}
\end{figure}


The distance plot now also looks a bit different compared to a fixed lr. We can
see that after an initial increase in distance to the checkpoint, the distance
reaches a local maximum and then actually decreases again until coming into an
area of convergence. For each restart performed, the same pattern repeats again.
For step decay, we can see hard edges in contrast to smooth curves of cosine
decay. The similarity to the alteration of the learning rate gives evidence that
the decrease of distance is caused by the learning rate. There are two possible
explanation: 

First of all, the decrease in learnig rate could lead to smaller increase in
distance by the pure fact, that smaller learning rate leads to smaller updates
of the weights and threfeore smaller increase in distance. However, this
couldn`t explain why the distance itself actually keeps decreasing, not just
smaller increasing. Only could try to explain this with the $L_2$ Loss and a
similar argument to section \ref{res:Multiple}. For high learning rate, a larger
Momentum builds up. This leads to parameter values, which are very larger. After
the momentum is slowed down, the $L_2$ regularization keeps decreasing the
weights again until reaching an equilibrium. Larger convergences in the
follwoing cycles make this explanation doubtful. 

Another idea could relate to the wedges. If we enter a wedge, it might be
possible that we end up on a circular path around the checkpoint. This might
lead to a valley which decreases the distance again. For large learning rate, we
can just jump between those valley, but for small learnig rates, we just follow
these valleys.

Another idea could relate to the loss landscape. In phases of high learning
rate, a momentum builds up and enforces larger weights. With smaller learning
rate, possible to take the path of small valleys where for large lr, just
follows overall  trend. Small valleys lead back to checkpoint.



We have seen that whenever a warm restart is performed, the accuracy drops.
However with decreasing learning rate, the network recovers and even tops the
performance of the previous cycle. This pattern is quite similar to effect of
multiple checkpoint of section \ref{res:Multiple}. Section
\ref{sub:Effect_on_Gradient} further motivates a comparison: We suggested that a
checkpoint would initially just increase the size of the gradient, rather than
changing its orientation, given that the orientation is stable over some epochs.
Therefore, the same effect on the parameter update, but rather from a larger
gradient itself than a larger learning rate, should be achieved.

Figure \ref{fig:Cosine_Multiple} shows a comparison. As mentioned, the pattern
is quite similar, with a drop in validation accuracy after the warm restart or
checkpoint. The top accuracy in each cycle also tops the last one for both
cases. The effect is similar with around 1\% increase in maximum accuracy
between the first and last cycle. However, the network with cosine decay gets a
much better absolute accuracy. That's probably due to the small learning rate at
the end of each cycle, where the network can exploit a small region to fine
adjust the parameter values.

\begin{figure}[h]\label{fig:Cosine_Multiple}
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch,
                ylabel=Validation Accuracy,
                ymin=0.75,
                legend pos = outer north east,
                xmin=-10,
                width=10cm]
                
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/MobileNetV2_scheduler_cosine_validation_acuracy.csv};
                    \addlegendentry{cosine decay}
                \addplot[mark=None, color=green] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/multiple/MobileNetV2/MobileNetV2_multiple_f100_validation_acuracy.csv};
                    \addlegendentry{multiple checkpoints}
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_validation_acuracy.csv};
                    \addlegendentry{baseline}
                
            \end{axis}            
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained with learning rate schedulers. [TODO: add for ResNet]}
    \end{center}
\end{figure}

Would a combination of both techniques lead to an even better result?
[TODO:add results and other factor] 


\subsubsection{wrong lr}

We might suspect that a learning rate decay should be more robust to a
suboptimal inital learning rate. At first, this seems to be case. Where a
network without decay only reaches a accuracy of 60\% and then decreases, a
scheduler reaches a comparable accuracy to the best network. But if we perform a
warm restart, the maximum accuracy drops for each cycle, so the network gets
worse with increasing cycles. If we add the distance function however, we can
see that it stabilizes the training so that the maximum accuracy stays the same
for each cycle. This leads to a performance difference of 8\% for MobileNetV2 in
favour of the distance function after 600 epochs. The difference is also present
from the beginning of each restart and remains stable. However, we loose the
effect from above, that each warm restart boosts the performance. Nevertheless,
this is a significant improvement, which is stable across both ResNet32 and
MobileNetV2.

\begin{figure}[h]\label{fig:Results_wrong_lr}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.3,
            legend pos = south east,
            xmin=-10]
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/lr1/MobileNetV2_scheduler_cosine_distance_lr1_validation_acuracy.csv};
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/lr1/MobileNetV2_scheduler_cosine_lr1_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/lr/MobileNetV2/run-mobileNetV2_baseline_distance_lr1_1-tag-Validation_accuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/lr/MobileNetV2/run-mobileNetV2_baseline_lr1_1-tag-Validation_accuracy.csv};
                % add for lr1 longer baseline
                

            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/lr1/MobileNetV2_scheduler_cosine_distance_lr1_distance0.csv};
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/scheduler/MobileNetV2/lr1/MobileNetV2_scheduler_cosine_lr1_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/lr/MobileNetV2/run-mobileNetV2_baseline_distance_lr1_1-tag-distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/lr/MobileNetV2/run-mobileNetV2_baseline_lr1_1-tag-distance0.csv};
                % add for lr1 baseline
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.7,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_validation_acuracy.csv};

            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained with learning rate schedulers. [TODO: add for ResNet]}
    \end{center}
\end{figure}

The effect can also be spotted if we compare without scheduler. Here, both
networks decrease in accuracy over time, but the distance term seems to reduce
that decrease. 




- show general improvement of scheduler
- step vs cosine
- combine both with distance show improvement
- compare to multiple as another method of gradient boost


- gradient:
    warm restart decreases size and increase again with restart
    but effect is reversed for wrong lr, maybe distance term leads to more consistent gradient size -> better performance
    but has to carefully look how large gradient size difference is 
    



result idea:
- difference with wrong lr
- too high doenst lead to convergence, too low leads to slower one
- if kernel is applied, starts distancing
- 
-
for scheduler:
- maybe sgd walks in area of low accuracy, cannot recover (what is this area)
- distance kernel helps escape from this area


\subsection{epochs}\label{res:Epochs}
epochs and epoch time

maybe not use since or only for scheduler and multiple, since with one not helpful

\section{Computational Cost}\label{Res:Computational_cost}

\section{ensemble methods}
- idea: uncorellated networks
- also make run without warm restart

results:
- first show higher accuracy than single network
- than see, that distance has no better value than without
- show that distancing doenst mean more diverse prediction
- implication for loss landscape -> everywhere the network behaves the same??



example picture:
\begin{figure}[h]\label{fig:MobileNetV2_baseline}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 1,
                horizontal sep=10pt,
                group name=G},
                width=8cm,
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.8]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_distance0.csv};
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
    
            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray (left) and Distance values (right) for a network trained without distance function (red) and with distance function (blue).}
    \end{center}
\end{figure}




have to do longer run of width and longer run of strength for highest factor at least, let distance kernel out again and look if comes back, add distance kernel for 0 epoch to show weight size