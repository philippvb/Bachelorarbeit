\chapter{Results}
Even with such a simple formulation, there are many different variations that
can be tested. First is the influence of the strength and width parameter of the
distance function, which will be tested in section \ref{res:Hyperparameters}. We
will also briefly test different kernels besides the RBF Kernel from equation
\ref{eq:RBF} in section \ref{res:Kernel}. In section \ref{res:Multiple}, we will
use more than one checkpoint and will also try to combine these to a global one.
The interaction of training hyperparameters like learning rate and epochs will
be discussed in section \ref{res:Training}. Finally, the usage for ensemble
methods will be tested.

\section{Baseline}
For the baseline, we use the hyperparameters as defined in section
\ref{sub:Hyperparameters}. Figure \ref{fig:Results_baseline} shows the result,
if we add a checkpoint after 100 epochs of training. For the validation accuray,
we can see that it stays similar to the accuracy without the distance function.
If we plot the distance the network gets to the checkpoint however, we can see
that the network trained with distance function clearly increases its distance
more than the one trained without. Furthermore, the red plot follows the shape
we would expect from the distance function. At the beginning, the distance
increases relatively fast, as can be expected from the high gradient of the RBF
Kernel. But as the distance increases, its gradient becomes smaller, similar to
the gradient of the Kernel. The blue plot in contrast follows an almost linear
curve, whith a much smaller gradient. Therefore, the additional term succeeds in
the initial goal, which was to distance from the checkpoint. Note that despite
the validation accuracy beeing in an area of convergence, SGD even without
distance function keeps on walking and never truly stops at a point.

\begin{figure}[h]\label{fig:Results_baseline}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm,
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.8,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_distance0.csv};
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.8,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/ResNet32_baseline_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/ResNet32_baseline_distance0.csv};
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained without distance function (red) and with distance function (blue).}
    \end{center}
\end{figure}







\section{Distance function Hyperparameters}\label{res:Hyperparameters}
\subsection{strength}
Recall section \ref{eq:LossDistance}, where we added strength as a
hyperparameter, controlling the influence of the distance function. As the RBF
Kernel is bound between 0 and 1, the strength hyperparameter will increase that
bound between 0 and the strength value.

For higher strength values, the validation accuracy receives an initial drop,
which is larger the higher the strength parameter value. This may be due to the
increased influence of the strength function. Each distance between the
parameters and the checkpoint starts at 0, therefore the values of the
exponential function starts at $strength \cdot 1$. A higher strength value will
lead to a higher influence on the loss function, and consequently the gradient.
The gradient of the cross-entropy loss will be insignificant, and the optimizer
will update the weights without regards to the validation accuracy, hence the
drop at the beginning. The distance plot reflects this behaviour, as we can see
an larger increase in distance for larger strength values. [add from wedge paper]

After the initial step however, the distance quickly converges. This is due to
the fact that after the initial increase, the value of the Kernel quickly
reaches 0. [TODO: Sow plot of strength]. Therefore, the distance Kernel becomes
insignificant again, and the Cross-Entropy loss is followed. The validation
accuracy reflects this behaviour, as after the initial drop the accuracy
recovers to the level of before. This provides additional insight in the loss
landscape. As the network distances from it`s current position on the loss
landscpae, but is still able to reach high accuracy, there have to be areas of
high accuracy everywhere on the landscape. This is in consonance to other
research in this field, as discussed in section \ref{loss_landscape}.

As a result, the value of the strength doesn't matter in long term at least. It
merely increases the distance to the checkpoint by defining how long the
distance term is important to the loss function. But after that, the normal loss
is followed again. As the last section and literature suggests, area of low loss
and high accuray can be found everywhere. Therefore, the same accuracy as before
can be reached, no matter the value of the strength. On the other hand, it is
also unlikely that it will outperform the last best value, as new areas are not
generally better than other.

basic results:
- if higher strength factor, distance rises more quickly
- then stays in this area of distance
- validation accuracy drops because first influenence of distance




\begin{figure}[h]\label{fig:Results_strength}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.6,
            legend pos= south east,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e1_validation_acuracy.csv};
            \addlegendentry{$s=0.1$}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            \addlegendentry{$s=1$}
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e2_validation_acuracy.csv};
            \addlegendentry{$s=10$}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e3_validation_acuracy.csv};
            \addlegendentry{$s=100$}

            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e1_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e2_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/MobileNetV2/MobileNetV2_strength_e3_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.6,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e1_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e2_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e3_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e1_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e2_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/strength/ResNet32/ResNet32_strength_e3_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained without distance function (red) and with distance function (blue).}
    \end{center}
\end{figure}

\subsection{width}
We have seen in chapter \ref{distance_function}, how the width $\sigma$
influences the distance function: The larger $\sigma$ gets, the wider it
becomes. Therefore, a larger $\sigma$ should result in the network distancing
further from it's checkpoint than for smaller values. That's exactly what can be
seen in figure \ref{}: For an $\sigma$ of 0.1, the distance follows the one of
network trained without distance function, due to the RBF Kernel quickly
becoming 0. The larger the width, the slower the values of the RBF Kernel
decrease for further distance. Therefore, the gradient of the distance function
will stay quite large, pushing the network further away. This can be seen for
higher $\sigma$ values, where the distance increases further.


Initially however, the distance plot doesn't reflect this behaviour. The curve
for $\sigma = 0.01$ has a higher distance value than for $\sigma = 0.001$.
That's because the gradient of the RBF Kernel is higher for small distances, the
smaller $\sigma$. But for larger distances, this effect flips with the gradient
of the larger width staying higher for more epochs. Therefore, the curves
intersect after additional epochs. The smaller width converges, while the larger
stays constant until eventually reaching it`s area of convergence. The
intersection happens later the larger the width, for $\sigma = 0.0001$, only
after 207 epochs.

This behaviour however has no influence on the validation accuracy, which stays
the same for every width, in contrast to the strength. This may be due to the
fact, that a larger width does in fact decreases the size of the gradient. A
smaller gradient over more epochs will be added to the gradient of the
Cross-Entropy Loss. The optimizer therefore nearly keeps on following the normal
gradient and descending the valley of low loss, with a small but steady push to
increase the distance to the checkpoint. Therefore, even if the network is
allowed to move quite freely on the loss surface, it finds areas of low loss and
high accuracy in distant areas of the checkpiont, meaning there has to be good
local minima everywhere on the landscape.

[mention witdh e-3 good behaviour in terms of tradeoff]

\begin{figure}[h]\label{fig:Results_strength}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 2,
                horizontal sep=10pt,
                vertical sep=10pt,
                group name=G},
                width=8cm
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            % xlabel=Epoch,
            ylabel=Validation Accuracy,
            xticklabels={,,},
            ymin=0.7,
            legend pos = south east,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e1_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=1$}
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e2_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=10$}
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=100$}
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e4_validation_acuracy.csv};
            \addlegendentry{$\sigma^2=1000$}

            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                % xlabel=Epoch,
                ylabel=Distance,
                yticklabel pos=right,
                xticklabels={,,},
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e1_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e2_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/MobileNetV2/MobileNetV2_width_e4_distance0.csv};
    

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.7,
            xmin=-10]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_validation_acuracy.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_validation_acuracy.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_validation_acuracy.csv};

            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e1_distance0.csv};
            \addplot[mark=None, color=orange] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e2_distance0.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/ResNet32_baseline_distance_distance0.csv};
            \addplot[mark=None, color=green] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/width/ResNet32/ResNet32_width_e4_distance0.csv};

            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray and Distance for MobileNetV2 (upper) and ResNet32 (lower) trained with different widths of the distance function.}
    \end{center}
\end{figure}








results: - wider kernel leads to higher value of
distance - maybe slower due to smaller gradient - no difference in validation
accuracy - if so, should prove that in every area exists low loss regions -
accuracy doesn't decrease -> there are valleys leading away from current point
with high accuracy - 







\section{Multiple Minimas}\label{res:Multiple}
idea: distance from multiple rather than 1 place
\subsection{multiple}
For multiple minima, we can see the same pattern as in section
\ref{res:Hyperparameters}. For low strength, we experience no effect at all. If
we increase the strength there is an initial drop in the accuracy, but with
further epochs, this recovers to the baseline accuracy. Furthermore, it even
tops the baseline, with a small but increasing margin for every restart that is
performed. The pattern is quite similar to warm restarts, where for every
restart the reached accuracy increases. For a in depth comparison, see section
\ref{res:Learning rate}.

The influence on the distance to the checkpoints is more interesting however. In
general, an additional checkpoint seems to decrease the distance to the others.
This is quite counterintuitive at first glance. Suppose for one parameter, we
have checkpoint value $a$ and create a new one at our current value $b<a$. To
icrease the distance to both, we would just have to walk in direction $c<b$. As
we have seen in previous section, this would be sufficient as areas of low error
exist everywhere. But why is the distance then decreased for our first
checkpoint? The reason might be the $L_2$ regularization, as it restricts the
weights to small sizes. At some point, it might be less costly to decrease the
weights again for a smaller $L_2$, traded off against a bit smaller distance to
other checkpoints. This restriction in weight space is probably why new terms
lead to a decrease of the distance to existing checkpoints.
[maybe long term convergence to each other]

In short term however, new checkpoint produce quite different influence on the
distance to existing ones. For a strength of $s=1$ we can see a slight decrease
which turns into a longer increase until the next checkpoint. For $s=10$, we an
aprupt decrease with an small peak after. For $s=100$, there only is a peak at
the beginning which quickly diminishes. Noticeable, this pattern repeats for the
other checkpoints and therefore seems unlikely to be a coincidence. The size of
these hills can be tracked to the size of the strength. Larger strength
introduces a larger gradient and therefore step size at the beginning, making
the hills larger. This doesn't explain the different orientation of these hills.
For the case of $s=100$, the initial increase of distance seems reasonable. As
the value of the distance function is not 0, the gradient should face in
direction away from the last checkpoint. A new checkpoint introduces a gradient
boost. As momentum preserves the old gradient, it will acclerate in direction
away from the first and the second checkpoint, therefore leading to a hill.

By this explanation, the pattern should repeat for the other cases however. It
may be argued, that this pattern would still exist for other values of s.
However, it is only existent in the case of large s.




- only effect for larger strength
- initial decrease, similar to starts
- reaches slightly lower train loss, but not better validation accuracy
- have to take longer baseline run to see if it comes over baseline accuracy

- for new checkpoints effect on distance of current, at the moment not consistent -> has to check for average
- in general, seems like a slight decrease but same pattern again for single checkpoint, but others show same general pattern
- explanation idea: new kernel pushes gradient size -> increase or decrease (should be an icrease because momentum???)

- gradient size becomes smaller for distance term -> maybe hint that SGD wants
to stay in that area and distance term tries to push out -> gradient direction
would be interesting (but too far for this work)



- show that multiple minima all increase distance
- show how it can be merged to a large one

- increase factor to show differences, also überleitung to warm restarts
\subsection{merge}

\section{Training Hyperparameters}\label{res:Training}
\subsection{lr}\label{res:Learning rate}
\subsubsection{Scheduler}
Learning rate Scheduler were introduced in section \ref{sub:Learing_rate_decay},
where the learning rate was not held constant, but decayed over time. One futher
addition were warm restarts, where after the decay we set the learning rate back
up to the inital and repeat this cycle several times.

This procedure results in an increase of validation accuracy. We use a step
decay scheduler with $\gamma = 0.1$ for every 50 epochs, and a warm restart
after 150 epochs. Additionally, a cosine decay is used as in
\cite{loshchilov2016sgdr}. Figure \ref{} shows the validation accuracy. We can
see, that schedulers outperform a fixed learning rate. Furthermore, the maximum
accuracy of every cycle slightly increases, until eventually coming to
convergence. We can clearly see the warm restarts, as the accuracy drops due to
the high learning rate.

The distance plot now also looks a bit different compared to a fixed lr. We can
see that after an initial increase in distance to the checkpoint, the distance
actually decreases again until coming in an area of convergence. For step decay,
we can see hard edges in contrast to smooth curves of cosine decay. The
similarity to the alteration of the learning rate gives evidence that the
decrease of distance is caused by the learning rate. There are two possible explanation: 

First of all, the decrease in learnig rate could lead to smaller increase in
distance by the pure fact, that smaller learning rate leads to smaller uodates
of the weights and threfeore smaller increase in distance. However, this
couldn`t explain why the distance itself actually keeps decreasing, not just
smaller increasing.

Another idea could relate to the wedges. If we enter a wedge, it might be
possible that we end up on a circular path around the checkpoint. This might
lead to a valley which decreases the distance again. For large learning rate, we
can just jump between those valley, but for small learnig rates, we just follow
these valleys.



We might suspect that a learning rate decay should be more robust to a
suboptimal inital learning rate. At first, this seems to be case. Where a
network without decay only reaches a accuracy of 60\% and then decreases, a
scheduler reaches a comparable accuracy to the best network. But if we perform a
warm restart, the maximum accuracy drops for each cycle, so the network gets
worse with increasing cycles. If we add the distance function however, we can
see that it stabilizes the training so that the maximum accuracy stays the same
for each cycle. However, we loose the effect from before, that each warm restart
boosts the performance. Nevertheless, this is a significant improvement, which
is stable across both ResNet32 and MobileNetV2.

The effect can also be spotted if we compare without scheduler. Here, both
networks decrease in accuracy over time, but the distance term seems to reduce
that decrease. [Explanation??]




- show general improvement of scheduler
- step vs cosine
- combine both with distance show improvement
- compare to multiple as another method of gradient boost


- gradient:
    warm restart decreases size and increase again with restart
    but effect is reversed for wrong lr, maybe distance term leads to more consistent gradient size -> better performance
    but has to carefully look how large gradient size difference is 
    

\subsubsection{wrong lr}


result idea:
- difference with wrong lr
- too high doenst lead to convergence, too low leads to slower one
- if kernel is applied, starts distancing
- 
-
for scheduler:
- maybe sgd walks in area of low accuracy, cannot recover (what is this area)
- distance kernel helps escape from this area


\subsection{epochs}\label{res:Epochs}
epochs and epoch time

maybe not use since or only for scheduler and multiple, since with one not helpful

\section{ensemble methods}
- idea: uncorellated networks
- also make run without warm restart

results:
- first show higher accuracy than single network
- than see, that distance has no better value than without
- show that distancing doenst mean more diverse prediction
- implication for loss landscape -> everywhere the network behaves the same??



example picture:
\begin{figure}[h]\label{fig:MobileNetV2_baseline}
    \begin{center}
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                group size=2 by 1,
                horizontal sep=10pt,
                group name=G},
                width=8cm,
            ]

            \nextgroupplot[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=Epoch, % Set the labels
            ylabel=Validation Accuracy,
            ymin=0.8]
            \addplot[mark=None, color=red] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_validation_acuracy.csv};
            \addplot[mark=None, color=blue] 
                table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_validation_acuracy.csv};
            
            \nextgroupplot[
                grid=major, 
                grid style={dashed,gray!30},
                xlabel=Epoch, % Set the labels
                ylabel=Distance,
                yticklabel pos=right,
                ylabel near ticks]
                \addplot[mark=None, color=red] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline/MobileNetV2_baseline_distance0.csv};
                \addplot[mark=None, color=blue] 
                    table[x=Step, y=Value, col sep=comma]{images/network_csv/baseline/baseline_distance/MobileNetV2_baseline_distance_distance0.csv};
    
            \end{groupplot}
        \end{tikzpicture}
        \caption{Validation accuray (left) and Distance values (right) for a network trained without distance function (red) and with distance function (blue).}
    \end{center}
\end{figure}




have to do longer run of width and longer run of strength for highest factor at least, let distance kernel out again and look if comes back, add distance kernel for 0 epoch to show weight size