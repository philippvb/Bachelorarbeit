\chapter{Dicussion}

In section \ref{sub:Motivation}, we summarized some of the challenges of
optimization and described our approach to overcome some of these. The key idea
was to push the network away from a checkpoint to escape local minima and
explore the loss landscpae further. The following sections described how we
tried to realize this goal: We expanded the loss function by a term which
measures the distance between the checkpoint and current state and penalizes
small distances.

The results showed that the distance function acted successfull at achieving
this goal. Whenever we included the distance term, the network distanced further
than without, due to the additional gradient. In section
\ref{res:Hyperparameters} we showed, how this distance can be modified. A wider
RBF Kernel lets the network distance further, but at a slower rate due to the
smaller gradient. A larger gradient is created by varying the strength factor.
Although this only increases the width of the Kernel by a small margin [add
figure], the distance increases very fast and strong. The faster increase is due
to the larger gradient. It seems like the increase is too strong compared to the
value,what may be due to momentum. This also shows that after an increase, an
area in the loss landscape has low loss in a sense that the network doesn't
want to come back.

When we add multiple checkpoints, we see that each new checkpoint decreases the
distance to the other again. This may be due to L2






Another interesting fact was that even without a distance Kernel,
SGD also increases the distance, even when the validation accuracy seems to
converge. This gives evidence for another difference between neural networks and
traditional optimization. Where in traditional optimization, the goal normally
is to reach global optimum, optimization in neural networks fails to do so.
Nevertheless, the good performance of optimizer like SGD shows, that it may not
even be necesarry or useful to search for a global optimum. The continous motion
of SGD also implies that traditional termination metrics like 0 gradients are
not sufficient, as the size of the gradients doesn't decay over time. Therefore,
other metrics like early stopping at a good validation accuracy have to be taken
into account.

However, recent research on loss landscape suggests that there are cells of
local minima which are connected by valleys of low loss. Between these cells
however lie areas of high loss. To overcome these areas of high loss and jump to
other cells, we used our distance function. Although we already saw that the
distance function was successfull in achieving its goal, this doens't transfer
to the accuracy. Whenever we created a checkpoint, we saw a drop in validation
accuracy. The higher the influence of the Kernel, the higher the drop was.
Section \ref{res:Hyperparameters} showed some reasons for this from a
mathematical viewpoint. Whenever the influence of the distance function is
higher, its gradient will also be larger compared to the gradient of the
cross-entropy-loss. Therefore, the gradient of the cross-entropy-loss will be
neglected and the accuracy drops. [showed that we didn't traverse valleys]
However, after further epochs, the accuracy recovered to the same level as
before. Combined with the distancing, this means that areas of low loss can be
found everywhere on the landscape. This is consistent with current literature
like in [literature]. However in most cases, the network with distance function
doesn't top the network without. Even if there are areas of low loss everywhere
on the landscape, one doesn't seem to be better than another. Therefore,
distancing may be possible, but most of the times not useful.


For multiple checkpoints however, there is a small increase with distance
function compared to without. Furthermore, the same pattern as in cosine decay
arises, where new cycles top the maximum accuracy of the previous. We suggested
that this might relate to the simliar influence on the gradient, that the
parameter updates just get larger at the beginning. An open question is still
why this effect happens at the first place for warm restarts. The idea of
exploring the landscape in phases of high learning rate seems appealing, but
then there should be no difference compared to our network. This would also
contrast the idea of areas of low loss everwhere on the landscape.

Another idea is that high learning rate couls leads into wider minimas. Large
steps may overshoot the minima itself, and also amplify errors in the gradient
arising from empirical risk. If the new gradient after the parameter update
however leads back to the same point, this has to mean that the area is
relatively wide, achieving a good performance. However when applying a distance
function, these local minimas dissappear. Therefore all parameter values
distance from the checkpoint, rather than only those with are unstable.


As this effect couldn't be
transferred to our method, it can provide an explanation for the difference. For
the distance function to induce a gradient boost at the beginning, the gradient
has to be stable in one direction. In contrast, warm restarts increase the step
size in every direction. So as we try to distance from a checkpoint, warm
restarts explores the area around a checkpoint, but stays at good parameter
values rather than distancing from all of them.




Hypothesis: Warm restart is better than us,because only distances from bad
parameter values. Can maybe also be seen in plot,where distance decreases again after lr decreases





- warm restart has phases of high learning rate, thatlead to a wider minimum -> better generalization
- key difference: wr relies on real gradient, while we create artificial
- wr is directioned to  larger steps rather than distacing, decreases its distance again for low learning rate
- so maybe not exploration but more stable minima might be the reason for the boost

%------------------------------------------------------
One major improvement of the distance function was for a wrong initial learning
rate. At first, it seems that a cosine decay should let the network be more
robust to a wrong learning rate because it is decayed until 0, irrelevant of the
inital learning rate. In section \ref{res:Learning rate}, this was the case
until the first warm restart. But after it was performed, both ResNet32 and
MobileNetV2 started diverging for a initial learning rate of 0.01 in a way that
the maximum learning rate of each new cycle decreased compared to the previous
one. In contrast when adding a distance function, the maximum accuracy stayed
constant.

There is no trivial explanation to this result. A simple approach would be, that
cosine decay with warm restart walks into an area of high loss, and is not able
to recover. The distance function then helps to distance from this point and
return to areas of lower loss, leading to a stable performance. However, this
would contradict other results from both and other paper. As the distance
function had no impact on the valdiation accuracy in section \ref{res:Baseline},
we suggested that this happens due to the fact, that areas of low loss exist
everywhere, which would contradict this explanation. Furthermore, it is unlikely
that both networks would walk into an area of high loss by chance.

-  maybe about gradient size
 

- wrong learning rate receives bad result for   cosine decay
- seems to come into an area of bad loss
-  escapes by  distance kernel
- but then why does it only happen there
- would notbe consistent with other findings on landscape



%--------------------------------------------------------
- ensemble methods are generally better
- but not much  increase even if we distance further
- reason: errors stay corellated
- other places in loss landscape not other predictions? states question why move landscape
- explanation 1: weight space symetry
- explanation 2: trivial and notrivial examples in CIFAR-10





















structure:

1. Sucessfullnes of results:
- distancing successfull
- have seen how we control distance with hyperparameters
- but that doesn`t increase valdiation accuracy

2 implications for loss landscape
- everywhere are areas of low loss -> implications for initilizations??
- that`s good, as we normally aren`t moving away from our cell
- but even in our cell, we keep on moving


3. ensemble methods:
- have seen, that networks predict the same regardless of position on loss landscape
- why is that? weight space symetry, dataset
- 


ideas what really happens:
- first kernel just boosts gradient in tunnel of low loss
- the direction should stay, doesn't change direction at wedge intersection, if kernel is wide enough
- too strong kernel lets leave the wedge
- lowering lr maybe opens the position to change at wedge intersection -> distance becomes smaller again
- ensembles may stay same due to not changing wedge -> but should then work for high strength values




summary: we have seen that although otimization seems nearly impossible at the
beginning, it becomes quite trivial in the following. This may be due to the
large number of local minimas, which can guarantee us a good accuracy in every
are. Moving between this areas doesn't normally happen. The question if this may
be even necessary arises for example from the comparison of the networks, which
has shown that even networks different in the weight space predict quite similar
to each other.

Other inutitions of normal optimization also dont apply, for
example finding local minimas doesn't seem to be necessary and sufficient, as
SGD nerly never stops walking.


other thoughts:
- because distance increases constantly even for multiple, maybe we stay on wedge