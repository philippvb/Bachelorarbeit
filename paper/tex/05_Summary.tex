\chapter{Summary}
In the beginning we have seen that the setting for optimization is very
challenging: A highly non-convex loss landscpape which is expensive to compute
and consists of a large number of local extrema. In practice however,
optimization becomes suprisingly easy, even simple optimizers like SGd reaching
good results. Current literature suggest that this may be due to that local
minima mostly have low loss, exist everywhere on the landscape and are some
connected by valleys. However, it appears that these connected minima are
concentrated as cells surrounded by areas of high loss.

Our motivation was to distance from a checkpoint on the landscape to overcome
these boundaries and explore the network further. We achieved this by adding a
distance termtothe loss function, which penalizes small distances to the
checkpoint. The results showed that this lead to a larger distance compared to
normal SGG or cosine decay. Furthermore the distance could be controlled by the
modification of the Kernel of the loss function. However, a further distance
didn't increase the validation accuracy. This lead to the conclusion, that most
local minima have approximately the same accuracy regardless their position on
the landscape.

If we add multiple checkpoints, there arised a similar pattern like cosine decay
with warm restart. We argued that this might be due to similar underlying
effects on the parameter updates. For a wrong learning rate, the addition of the
distance function stabilizes the training process. As this was limited to a
particular learning rate, there was no trivial explanation to that.

Finally, we applyied the distance function to ensemble methods, with the idea of
more uncorellated snapshots from a larger distance on the loss landscape.
However, the corellation seemed to stay the same regardless of the distance.
This questions the idea of ensemble methods, even if they perform well.
Furthermore it was showed that although a low corellation may be desireable, it
cannot recover a low individual performance. As high performance means higher corellation, this further ...


In summary, although distancing is certainly possible, it seems not to achieve a
better network performance. That happens because the loss landscape contains of
local minima with same cost everywhere. Further work in this area should focus
on exploring the landscape more precise and looking for implications on other
reseachin this field like initialization strategies.