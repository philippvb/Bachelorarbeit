\chapter{Methods}
\section{Distance Function}
\subsection{Motivation}
In section \ref{loss_landscape}, we got a brief overview over the loss landscape
of deep neural networks and the resulting challenges for optimization. Although
there exists a large number of local minima, most of them have low cost.
Furthermore, they are arranged in cells, where each minima has low cost and is
connected to the others via valleys of low loss.

These cells create a challenge for optimization. Suppose the learning gets into
the area of one of these cells. As mentioned in chapter \ref{loss_landscape} the
cells are surrounded by nondifferential boundaries and areas of higher loss.
[TODO: where is this stated?] This will likely cause the algorithm to get stuck
into this cell. As all of the connected minimas in this cell are of approximatly
same loss, there will be a boundary until the algorithm can improve, which may
be higher than the global optimum. This imposes the question, if continuing to
train is useful, as after one of the minima of the cell is found, the nearby
minima it can reach will offer no significant improvement and other cells are
out of reach.

If we reuse the ball analogy from section \ref{sub:Momentum}, the beginning of
the training process would probably place the ball at the top of a mountain
landscape. The initial training let's the ball move into one of the valleys.
This valley may connect the ball to other points of low altitude, like minima
are connected in a cell. When the ball is only allowed to move downhill however,
the ball can never reach a spot which he is seperated by a hill. Thus, his
minimum altitude is bound to the lowest place in his current valley. If we add
momentum, we have seen that the ball is able to get over hills of certain size
by using his momentum. Warm restarts add the possibility to jump over a hill
with a high learning rate at each restart. This can lead to an escape of the
cell, but without guarantee. What is more likely to happen is that after a large
step, the ball is in an area of higher altitude, which will let him roll down to
the same valley again in the next step.

In contrast, we try a different approach. Rather than letting the ball make one
large steps and then allwoing him to move on freely, we want to constantly push
the ball away from it's current position. The idea beeing if the ball is only
surrounded by hills, we have to push it up one of these until it reaches the top
and can then roll down into another valley, therefore escaping the cell.

Transferred back to a real network, this translates to repeatedly updating the
parameter values in a way, that the new values will increase their distance to
the values we want to get away from.

In Pseudo-Code, this looks the following:
\begin{algorithm}
    \begin{algorithmic}[1]
        \caption{Network training with distancing}
        \REQUIRE a neural network architecture and a dataset
        \ENSURE a trained neural network
        \STATE initialize the network, dataset and training parameters
        \FOR{$i \leftarrow 1$ \textbf{to} desired number of epochs}
            \STATE compute foward and backward pass of training data
            \STATE update parameter values with optimizer
        \ENDFOR
        \STATE create checkpoint we want to distance from
        \FOR{$i \leftarrow$ next epoch \textbf{to} end}
            \STATE compute new parameter values different to checkpoint
            \STATE update parameter values with optimizer
        \ENDFOR
        \STATE \textbf{return: the trained network}
    \end{algorithmic}
\end{algorithm}

First, we train the network the traiditional way - we compute the foward and
backward pass of the training data with the resulting gradient, and then update
the parameter values with an optimizer. After we have reached a sufficiently low
error, we create the point we want to push away from, which we call
\textbf{checkpoint}. The difference in the next training loop in contrast to the
standard one at the beginning is how the parameter values are computed. Here,
our goal is to distance from the checkpoint. This will also result in a
gradient, which will be applied the same way as above.

How can we encourage a network to distance from a checkpoint, while at the same
time not sacrificing the networks performance. Back in the ball analogy, instead
of pushing the ball up a hill, we could place a hill at the current position,
and let the ball roll down. Figure \ref{fig:Distance2D} shows a graphical
illustration for the 2D case. An advantage of this technique is that the hill
will only change the loss landscape locally. So when we have distanced from the
hill, we follow the normal loss landscape again.

\begin{figure}[h!]\label{fig:Distance2D}
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=x, % Set the labels
            ylabel=y,
            xmin=-2,
            xmax=3,
            xtick={-2,-1,0,1,2, 3},
            ymin=-2,
            ymax=3,
            samples=100,
            legend pos = north west,
            width=0.7\textwidth,
            legend style={nodes={scale=0.8, transform shape}}]
            \addplot[color=red]{x^3-x^2};
            \addlegendentry{$f(x)=x^3-x^2$}
            \addplot[color=blue]{exp(-(x-2/3)^2)};
            \addlegendentry{$g(x)=exp(-(x-\frac{2}{3})^2)$}
            \addplot[color=black]{x^3-x^2+exp(-(x-2/3)^2)};
            \addlegendentry{$h(x)=f(x)+g(x)$}
            \end{axis}
         \end{tikzpicture}
        \caption{Example of a function with a local minimum at $x=\frac{2}{3}$. If we place an Gaussian function on top, the minimum disappears. Note, that the black function approaches the red if we distance from the local minimum.}
    \end{center}
\end{figure}

Formally, this hill is realised by first computing the distance of the parameter
values to the checkpoint, and then adding a penalty for small distances. If we
use and exponential Kernel, the idea of the hill can be taken quite visually.
Section \ref{sub:Mathematical_approach} describes the mathematical approach in
detail.

What happens when our penalty hill is not large enough to get the ball over the
hill? This means we would get back to the present parameter values. That's not
necesarryly a problem, as it would mean that our current position is very
robust, which would lead to a good generalization capability. Therefore our
network would stay in areas with stable minima, which is a desired property.

For ensemble methods, there are other possible benefits of this approach. Recall
that for an ensemble to perform well, the errors of the participating networks
should be uncorellated. SGD with warm restart \cite{loshchilov2016sgdr} tried to
ensure this with warm restarts, where a high learnin rate increases the distance
of the networks and therefore leads to uncorellated errors. We could use our
method as alternative approach, where we ensure the distance between the
networks not only by high learning rate but by explicitly increasing the
distance. Of course, the underlying idea for both is that networks which are
more distant in parameter space will behave more differently, and therefore have
more uncorellated errors.




\begin{comment}

How to overcome this issue? One solution would be to escape these cells and move
to the next one, hopefully with lower cost. This can be achieved with the use of
warm restarts, as mentioned in chapter \ref{cosine_decay}. In phases of low
learning rate, the algorithm exploits the area of a cell. When the learning rate
is set back up to the inital learning rate, the update steps of the parameters
get much larger. If the update steps are large enough, this may lead to an
escape of the cell. However there is no guarantee, because this method relies on
the gradient beeing large enough.

From a naive point of view, it would be easiest to remember to place of the
cells, and just move away from it. In a simplified version, thats what our
algorithm does. It remembers the point which it wants to distance from, and then
rewards doing so. [describe better]
 

motivations:
- different values for ensemble method
- seen that cells exists, escape these cells
- explore instead of exploit

\subsection{Approach}
An easy approach to move away from an area would be to remember the point you
want to distance from, and then move in whatever direction increases this
distance. In a sense, this is what our algorithm does. First, we train the
network until there is no improvement anymore. Then, we remember the current
parameter values as a checkpoint. The network will now be encouraged to distance
from this checkpoint, and we keep on training. We can also repeat this procedure
several times.

The ball analogy from section \ref{sub:Momentum} can also be applied to
visualize the idea. Consider a ball in a mountain landscape. If we start at a
random location, the ball will roll down until it reaches the bottom of a
valley. What the distance function now does, is place a hill at the position
where the ball has stopped. Thereby, the ball will start to move away from the
current position again.

Note also, that we still use the normal loss function from the beginning in
combination. If we would only use the distance function, the network would
succeed in distancing from the checkpoint but fail to maintain the a good
performance. That's why we have to combine both terms to account for both
distancing and performance.
\end{comment}



\subsection{Mathematical approach}\label{sub:Mathematical_approach}
\subsubsection{Distance function}\label{distance_function}
As we want to measure the distance between two points, we need to define a
distance function. A common choice is the euclidean distance also know as $L_2$
norm, which is defined as: 
\begin{align}
    \rVert x \lVert_2 = \sqrt{\sum_i \lvert x_i \rvert^2}
\end{align}
This norm can also be squared to get rid of the root. Squaring does not change
the direction of the gradient, and is therefore possible. To measure the
distance between two parameter states $\theta_1$ and $\theta_2$ of the networks,
this results in:
\begin{align}\label{eq:distance}
    d(\theta_1, \theta_2)= \sum_i (\theta_{1_i}-\theta_{2_i})^2
\end{align}
The size and shape of the paramters $\theta$ is not important, as each paramter
is only compared to another state of itself, and is combined via a sum.

Another property is that the values the distance function can take is partly
dependend on size. Consider two networks $\theta_1$, $\theta_2$ with the same
classification task, but the size of $\theta_1$ is larger than $\theta_2$.[may
be confusing with before] If we assume all of the parameters are distributed the
same way, then $\theta_1$ would output a larger distance than $\theta_2$.
However it would be desirable for the functions to be in the same bound, as it
would make the transfer of hyperparameters for example possible. That's why we
use a function to control for the output to be in a certain bound.

If we take a look at support vector machines, they use kernels to compute the
similarity between two samples. One popular choice is the radial basis function
kernel, defined as:
\begin{align}\label{eq:RBF}
    k(\theta_1, \theta_2)=exp(-\frac{\rVert \theta_1 - \theta_2 \lVert^2}{2\sigma^2})
\end{align}
where $exp$ is the exponential function. With the use of \ref{eq:distance}, we
can convert this to:
\begin{align}\label{eq:DistanceFinal}
    k(\theta_1, \theta_2)=exp(-\frac{d(\theta_1, \theta_2)}{2\sigma^2})
\end{align}
If we plot this function in the two dimensional case (see Figure
\ref{fig:Gaussian}), we can see some nice properties.
\begin{figure}[h]\label{fig:Gaussian}
    \centering
    \textbf{Gaussian curve}\par\medskip
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
            grid=major, 
            grid style={dashed,gray!30},
            xlabel=x, % Set the labels
            ylabel=y,
            xmin=-20,
            xmax=20,
            xtick={-20,-15,-10,-5,0,5,10,15,20},
            ytick={0,0.5,1},
            ymin=-0.25,
            ymax=1.25,
            samples=200,
            legend pos = north west,
            width=0.7\textwidth,
            legend style={nodes={scale=1, transform shape}},
            domain=-20:20]
            \addplot[color=red]{exp(-x^2)};
            \addlegendentry{$\sigma=1$}
            \addplot[color=blue]{exp(-x^2/5^2)};
            \addlegendentry{$\sigma=5$}
            \addplot[color=green]{exp(-x^2/10^2)};
            \addlegendentry{$\sigma=10$}
            \addplot[color=black]{0};
            \end{axis}
         \end{tikzpicture}
         \caption{Plot of the function $f(x)=exp(-\frac{x^2}{\sigma^2})$ for different values of $\sigma$.}
    \end{center}
\end{figure}
First of all, the values are now bound between 0 an 1, regardless of the size of
$\theta$. Second, we can see as the values of the distance get larger, the
function approaches 0 asymtotically. This leads to a really small gradient for
extreme distances. Consequently, the Kernel will initially push the parameters
away from the checkpoint, but when this is achieved, will have little influence
on the loss function. How far the function encourages to distance from the
checkpoint can be controlled by the parameter $\sigma$, which defines the width
of the function and can be tuned as a hyperparamter. As $\sigma$ gets larger,
the function becomes wider. Therefore, the influence of the distance function
will reach further the larger $\sigma$ is.


\subsubsection{Loss function}
This section will show how the distance function from \ref{distance_function} is
combined with the normal loss function. The state of the art loss function for
image classification, which will be used for testing is the cross-entropy-loss
defined as:
\begin{align}
    -\sum_{i} \delta_{yc} log(f(x)_i)
\end{align}
Where $\delta$ is the Kronecker-Delta function defined as:
\begin{align}
    \delta_{xy} =
    \begin{cases}
        1 \textrm{, if } x=y \\
        0 \textrm{ otherwise}
    \end{cases}
\end{align}

To account for the distance term, we just sum it with the cross-entropy-loss:
\begin{align}
    L=\sum_{i} \delta_{yi} log(f(x)_i) + distance(\theta, \theta_c)
\end{align}
Where $\theta_c$ is the checkpoint.
Note that we can do this multiple times, so we can incooperate multiple
checkpoints:
\begin{align}
    L=\sum_{i} \delta_{yi} log(f(x)_i) + \sum_c distance(\theta, \theta_c)
\end{align}
 When computing the derivative for the backpropagation, the sum
decomposes into two terms, so the cross-entropy-loss will be computed the same
as before. Another property we want to control for is the influence of the
distance versus the cross-entropy-loss. When training is in later stages, the
cross-entropy-loss may be very small. If the values of the distance function a
too large in comparison, this would cause the parameters to be updated only
based on the distance, which is undesirable as the performance wouldn't be taken
into account anymore. The same is also true the other way around, if the
distance function is too small, it wouldn't affect training at all. That's why
we introduce a hyperparameters $s$ called strength to control this:
\begin{align}\label{eq:LossDistance}
    L=\sum_{c} \delta_{yc} log(f(x)_c) + s \cdot distance(\theta, \theta_c)
\end{align}

\subsubsection{Effect on Gradient}

\subsection{Pytorch implementation}

\subsubsection{Checkpoint creation}
To measure the distance, we have to create the checkpoint. The model parameters
in pytorch are stored as matrices for each layer and can be accesed via
$model.parameters()$, which outputs an iterable. We therefore opt to keep this
structure and safe the parameters in a list.
\begin{algorithm}[h!]
    \caption{Checkpoint}\label{alg:Checkpoint}
    \lstset{language=Python}
    \lstinputlisting{src/createCheckpoint.py}
\end{algorithm}
\newline
First, the checkpoint list is initialized. Then we iterate over the model
parameters. For each layer, we have to clone the parameters in order to create
new variables, and not just pointers to the existing ones. In addition they have
to be detached, to remove connection of the gradient to the model parameters.

\subsubsection{Distance function}
The L2 norm is implemented the follwing way: We iterate over the checkpoint and
the current parameter values. For each layer, we compute the difference, and
then square and add the values to our distance.
\begin{algorithm}[h!]
    \caption{L2 norm}\label{alg:L2Norm}
    \lstset{language=Python}
    \lstinputlisting{src/L2.py}
\end{algorithm}



\section{Configuration}

\subsection{Library and Training}
The code for the network was written in Python [Version], with the use of
Pytorch \cite{NEURIPS2019_9015} as the machine learning library. Data that occured during training was
logged and plotted with Tensorboard. For a detailed overview of the code, see
Appendix [add]. Training was done on the TCML Cluster of the university of TÃ¼bingen.

\subsection{Datasets}
Two common datasets were used.
\begin{itemize}
    \item MNIST \\
    MNIST is a classical example of hand-written digits from 0 to 9 that have to
    be classified accordingly. It contains a train set of 60000 and a test set
    of 10000 examples.
    \item CIFAR-10
    This dataset consists of 60000 32x32 colour images with 10 different
    classes. The dataset is seperated into 50000 train and 10000 test images.
    The state of the art accuracy is 97.3\%, reported from Kolesnikov et al.
    (2019). The dataset can be accessed via https://www.cs.toronto.edu/~kriz/cifar.html.
\end{itemize}
\subsection{Networks}
\subsubsection{ResNet}
The Residual Network (ResNet) architecture was first proposed by He et al.
\cite{he2016deep}. The idea of the ResNet was, that it is more easy for a
network to learn the residuals rather than the full transformation of an input.
Formally, consider the input $x$. The network transforms $x$ accoring to $H(x)$.
Rather than letting the network do the full transformation, ResNet produces
$H(x)= x +f(x)$, where $f(x)$ is the residual transformation learned by the
network, and x the identity data which is realized by a skip connection. Figure
\ref{fig:Residual_Block} shows a graphical illustration. Beside the assumably
easier to learn representation, ResNet also solves other problems. The issue of
the vanishing gradient as discussed in section \ref{sub:Vanishing_gradient} for
example is tackled, as skip connections backpropagate the gradient better to
earlier layers of the network. This allows for deeper networks.

\begin{figure}[h]\label{fig:Residual_Block}
    \centering
    \includegraphics[width=0.6\textwidth]{images/Residual_Block.png}
    \caption{Residual Block from \cite[Page 2]{he2016deep}\newline 
   }
\end{figure}

ResNet creates basic building blocks by applying a convolution, a Relu and
another convolution before adding the skip connection with a final Relu. Figure
\ref{fig:Residual_Block} illustrates this block. The blocks are then stacked
after each other. To let the network learn a more complex representation, after
a number of block, they increase the number of channels. Here, the identity
mapping of the skip connection is replaced by a pointwise convolution to
increase the number of channels. Alternatively, the new channels are padded with
0.



\subsubsection{MobileNetV2}
MobileNet was first introduced by Howard et. al \cite{howard2017mobilenets} as a
lightweight neural network for the use on mobile devices. The reduce the
computation effort of the network, the made use of the depth-wise seperable
convolution. Consider a 32x32x3 image of the CIFAR-10 dataset. A traditional 3x3
convolution with a stride and padding of 1 would produce an output of
$\frac{32+2-3}{1}+1$ the shape 32x32x1. To get more channels, we would need more
kernels. For a total of k output channels, we would need to do 3x3x3 x32x32 xk
multiplications. Instead of doing the computation in one kernel, depth-wise
seperable convolution divides it into two kernels. First they perform a
depthwise convolution, where a kernel of 3x3x1 is applied to every channel of
the input, so in this case we end up same size of 32x32x3. To get to k channels,
they use a pointwise convolution across the channels. This is a 1x1x3
convolution, which upscales the image to more channels. So if we want k output
channels, we need k 1x1x3 kernels. The benefit of this technique is that, while
getting the same output size, we only need 32x32 3x3x3 multiplications in the
first step, and 32x32 1x1x3 xk in the second step. This is way less than the
from the beginning. Figure \ref{fig:DSConv} shows a graphical illustration of
this idea.
\begin{figure}[h!]\label{fig:DSConv}
    \centering
    \includegraphics[width=0.6\textwidth]{images/Depthwise_Separable_Convolution.png}
    \caption{Depthwise Separable Convolution from \cite[Page 3]{sandler2018mobilenetv2} \newline On the left,
     a regular convolution with a 3x3xDepth kernel is drwan. On the right, this
     convolution is separated in depthwise convolution with a 3x3x1 Kernel,
     followed by a pointwise convolution with a 1x1xDepth Kernel. This methods
     saves computation time over the standard method, while also allowing
     interactions between channels.}
\end{figure}


With the second iteration called MobileNetV2 \cite{sandler2018mobilenetv2}, they
added the concept of inverted residuals or as they call it bottlenecks. The
ideas is, that intermediate representations should be low rather than high
dimensional. The bottleneck layer starts with a matrix with a small number of
depth channels. In the first step, the representation is expanded by a pointwise
convolution, followed by a Relu6. The expansion factor here controls how much
channels the intermediate representation will have. Then a depthwise convolution
is applied as an transformation step, followed again by a Relu6. Finally, a
pointwise convolution is applied again, but this time without a nonlinear
transformation following afterwards. The idea is that when compressing the data
back to a low dimensional space, a nonlinearity would only lead to a loss of
information. Finally, a skip connection from the input to the output is applied
to get a residual mapping from input to output.

% Figure x describes the network in detail.


For ImageNet, MobileNetV2 reaches an top-1 accuracy of 72\%, which outperforms
the competitors like MobileNetV1 or ShuffleNet 1.5 while having a similar number
of parameters.



\begin{comment}
    Further aspects:
    - MobileNet and ResNet add detailed structure
    - More about Kernel
    - How distance function will affect gradient more formally
\end{comment}



\begin{figure}[h]\label{fig:InvRes}
    \centering
    \includegraphics[width=0.6\textwidth]{images/Inverted_Residual.png}
    \caption{Inverted Residual Block from \cite[Page 3]{sandler2018mobilenetv2}\newline 
    First, a Relu 6 followed by a Pointwise Convolution is applied to map the
     data to a higher dimensional space. Then a depthwise convolution is
     applied, followed again by a Relu6. Finally, the data is compressed back to
     the original size. This time, there is no Relu applied to avoid the loss of
     information.}
\end{figure}


\subsection{Network hyperparameters and training loop}\label{sub:Hyperparameters}
For both networks, the base hyperparameters are the same. Note that these
parameters are only standard configurations and may be changed to investigate
the effects:
\begin{itemize}
    \item Optimization Algorithm \\Stochastic Gradient descent combined with
    Momentum is used. A $\lambda$ of 0.9 is set for Momentum.
    \item Learning rate \\Initially 1e-2
    \item Batch size \\128
    \item Regularization \\An $L_2$ Regularization as described in chapter
    \ref{sub:Generalization} is used with a factor of 1e-3.
    \item Loss Function \\Cross-Entropy Loss
    \item Number of epochs \\The number of epochs varies. As this is an
    explorative analysis, the training is often run for a large number of
    epochs, to investigate long term changes. Usually an epoch number of 900 is
    used.
\end{itemize}

\begin{comment}
\subsection{Hardware}
\begin{figure}[h]
    \centering
    \includegraphics{images/test.pdf}
    \caption{Feature Selection Based on Mutual Information}
    \label{fig:fs_mi}
\end{figure}


\begin{figure}[h]
    \centering
    \includesvg{images/nn.svg}
    \label{fig:fs_mi}
\end{figure}

\end{comment}

