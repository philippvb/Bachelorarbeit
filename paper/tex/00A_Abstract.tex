Current research suggests that most local minima on the loss landscape of deep
neural networks are of low loss and interconnected by paths
\cite{choromanska2015loss} \cite{draxler2018essentially} \cite{he2020piecewise}.
Nevertheless, Cosine Decay with Warm Restart \cite{loshchilov2016sgdr} has shown
that it can be beneficial to explore the loss landscape further. This work
follows up on the idea, but rather forces the exploration more directly by the
addition of a distance term to the loss function. Visually, the distance
function places a hill at the current position, called checkpoint, on the loss
landscape. This approach leads to a further distance to the checkpoint in
comparison to a standard loss function. In general, the effect does not
translate to validation accuracy, which seems to confirm the view, that areas of
low loss exist everywhere and are of same cost. For the addition of multiple
checkpoints, there was a small increasement similar to Cosine Decay with Warm
Restarts, probably to the same effect on the gradient. The distance term also
stabilized training with schedulers for a suboptimal initial learning rate.
Finally, the idea was tested in combination with snapshot ensembling, to reduce
the correlation between the snapshots. However, further distance did not imply
lower correlation, which questions the connection between the loss landscape and
behaviour of the network.