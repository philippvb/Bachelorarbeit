documentation of used networks:



be careful:
optimization in pytorch minimizes to 0

MNIST:
- used MLP from SLS
- Epochs based on:
    https://www.tensorflow.org/tutorials/keras/classification
    https://www.tensorflow.org/tutorials/images/classification
    also in graphs seems like 20 is enough maybe even 15

- wdecay:
    0.001 as used in https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting
    (rough number is gotten from dividing by number of batch examples: 128)
    also tried 0.01 which is worse, 0.0001 roughly same loss and acc -> take model with more reguralization

- lr: 1e-2 is best (tried 1e-1 to 1e-4)

"mnist1":{"dataset":"mnist",
            "model":"mlp",
            "loss_func": {"name":"softmax_loss", "distance":False, "factor":0.001},
            "optimizer":{"name":"sgd_momentum_wdecay", "lr":1e-2, "regularization":0.001},
            "acc_func":"softmax_accuracy",
            "batch_size":128,
            "max_epochs":20}


- Distance:
    used RBF Kernel
    
    first tried Variation of strength factor:
    the higher the value, the more distant to the last minimum the network becomes
    seems to have no influence on performance (at least in mid term)
    as distance converges, seems to converge also in accuracy -> does it really descent along valleys or just random walks and then converges??

    variation of width
    the width seems to control how fast a network converges, but also how strong? connection to strength


    theroretical influence of width and strength:
    strength should control how fast a network converges, but not that much at what value
    width should determine how far away a network converges, and also harm the speed of distancing


-> works well on mnist, but need to go to cifar to see if it really follows valleys or just randomly moves to new point and then converges



CIFAR-10

- used ResNet-34 from SLS
- hyperparams:
    lr:1e-3 best, 1e-2 quite similar but more unstable
    epochs: 60 seems appropiate maybe also 70
    regularization factor: 1e-3 only used maybe also test 1e-2 and e-4 to prevent from getting too high
    chose e-2 for convergence of distance


"cifar10_1":{"dataset":"cifar10",
            "model":"resnet34",
            "loss_func": {"name":"softmax_loss", "distance":False, "factor":1, "width":1},
            "optimizer":{"name":"sgd_momentum_wdecay", "lr":1e-3, "regularization":0.001},
            "acc_func":{"name":"softmax_accuracy"},
            "batch_size":128,
            "max_epochs":100}

- distance:
    1e-2 seems best convergence factor, 1e-1 converges too low
    maybe also 1e-3 has to look for longer run (maybe nearly no gradient so does take very long)
    -> solution seems to add a factor in front to let gradient get higher while let distance stay roughly the same

    no matter the method validation accuracy stays rougly the same to baseline model


- learning rate schedule:
    let drop by 0.1 every 50 epochs
    seems to increase validation accuracy
    first drop let distance decrease again, second lets distance get to convergence, why?

    validation accuracy same to baseline model, only w3_f2 is worse





possible next steps:
incorporate multiple minima
ensemble
see if it traverses valleys


- better when each minima is weighted the same
- seems to boost again


next week:
- train MobileNet further
- train ResNet further
- other kernel? 
- make large minima instead of multiple small
- learnrate drop trennt von distanzdrop
- back without scheduler?
- compare benachmarks for mboileNetV2 and train like them
- cosine decay mit warm restart statt mein scheduler


sbatch:
- long term MobileNet both with and without distance (1000 epochs)
- small test for other factors
- ResNet long term both (500?) or even 1000
- mobilenet benachmarks
- tanh kernel
- (resnet or mobilenet without scheduler or time offset just to see distance drop)


- resnet no distance with right lr
- mobilenet v2 1500
- mobilenet large minimum
- other kernel?




- mobilenet merge
- mobilenet tanh
- resnet no distance


clone mobilenet
initial network anderes training oder direkt mit kernel!!!
merge machen
zeit in tensorboard [DONE]

bessere Baseline 

multiple gpus pytorch [DONE]
cifar 100???


grundlagenteil
related work siehe cosine decay
wie man eine bachelorarbeit schreibt im netz suchen
aufbau sanduhr: abstrakter anfang und dann immer detailierter, zum ende wieder offen, auch in absätzen
referenzen: mit bibtex
zitieren: mit google scholar unten meistens bibtex
für sprache: von deepl zweimal übersetzen lassen, dann bessere formulierung



mobilenet 800 epoch direkt training
mobilenet 800 epoch baseline
mobilenet 800 epoch merge

mobilenet 800 richtig, wahrscheinlich hebt direktes minimum l2 auf


merge neue runs
clonen mobilenet
resnet 32 runs
weniger regularization



nimm falsche lr und schau mit distanz
merge nochmal (metrics neu!!!) für mobile und res
kernel mehr gewichte!!! vlt als zusammenhang mit lr scheduler
regularisierung
lass lr klein und nehme nur distanzfunktion

normalize gradient descent??

überblick machen

schlechtes netzwerk nehmen



nächste woche:
- bei lr1 kann man unterscheid replizieren?
- noscheduler nochmal
- ensemble methods? 
- factor mit cosine
- intervalle länger
- lr später kleiner bei distanz
- intervalle länger auch für lr 0.1
- lr 0.1 auch resnet
- run ohne regularisierung ohne distanz



- überschriften für methoden ergebnis teil



morgen vormittag:
überschriften checken
ergebnisse auswerten
einleitung erweitern und verbessern




- distance statt cosine decay bei kleiner lr
- pgf plot latex




results structure:

1.Baseline
show baseline with and without distance function, no learning rate decay, just fixed one

2. Parameters
    2.1 width
    use different widths with fixed lr
    2.2 strength
    use different strength with fixed lr
    2.3 interaction?
    show interaction of width and strength?? maybe for better runtime

3. Hyperparameters (maybe also after 4., depending on importance of multiple minimas for lr)
    6.1 lr
    show how it stabilizes lr

    6.2 number of epochs
    if more epochs, not better


4. multiple minima
    4.1 multiple
    multiple minima, fixed lr, maybe two combinations?
    4.2 merge
    merge into one large, fixed lr, maybe multiple merge combinations

5. Warm restarts
use one and multiple with warm restarts, maybe vary width
compare to distance function as warm restart (gradient boost at the beginning)



7. ensemble methods
show ensemble from cosine decay and trained with distance function, maybe also without lr scheduler

Fragen für morgen:
- curve smoothing?
- intrpretation schon in results?
- plot gradient size??
- plot gradient direction in comparison to last one??


z faktor ode pgf plot reihenfolge














